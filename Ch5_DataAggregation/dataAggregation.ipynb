{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation and Group Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorizing a data set and applying a function to each group, whether an aggregation or transformation, is often a critical component of a data analysis workflow.\n",
    "\n",
    "pandas provides a flexible and high-performance groupby facility, enabling you to slice and dice, and summarize data sets in a natural way.\n",
    "\n",
    "- Split a pandas object into pieces using one or more keys\n",
    "- Computing group summary statistics\n",
    "- Apply a varying set of functions to each column of a DataFrame\n",
    "- Apply within-group transformations or other manipulations\n",
    "- Compute pivot tables and cross-tabulations\n",
    "- Perform quantile analysis and other data-derived group analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "from numpy import nan as NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy Mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*split-apply-combine*\n",
    "\n",
    "- In the first stage of the process, data contained in a pandas object, whether a Series, DataFrame, or otherwise, is split into groups based on one or more keys that you provide. The splitting is performed on a particular axis of an object.\n",
    "- A function is applied to each group, producing a new value.\n",
    "- Finally, the results of all those function applications are combined into a result object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Pics/picture_5_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each grouping key can take many forms, and the keys do not have to be all of the same type:\n",
    "- A list or array of values that is the same length as the axis being grouped\n",
    "- A value indicating a column name in a DataFrame\n",
    "- A dict or Series giving a correspondence between the values on the axis being grouped and the group names\n",
    "- A function to be invoked on the axis index or the individual labels in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],\n",
    "                'key2' : ['one', 'two', 'one', 'two', 'one'],\n",
    "                'data1' : np.random.randn(5),\n",
    "                'data2' : np.random.randn(5)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted to compute the mean of the **data1** column using the groups labels from **key1**. One is to access **data1** and call **groupby** with the column (a Series) at **key1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df['data1'].groupby(df['key1'])\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **grouped** variable is now a *GroupBy* object. It has not actually computed anything yet except for some intermediate data about the group key **df['key1']**. The idea is that this object has all of the information needed to then apply some operation to each of the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data (a Series) has been aggregated according to the group key, producing a new Series that is now indexed by the unique values in the **key1** column. The result index has the name **'key1'** because the DataFrame column **df['key1']** did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we had passed multiple arrays as a list,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df['data1'].groupby([df['key1'], df['key2']]).mean()\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grouped the data using two keys, and the resulting Series now has a hierarchical index consisting of the unique pairs of keys observed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The group keys are all Series, though they could be any arrays of the right length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio']);\n",
    "years = np.array([2005, 2005, 2006, 2005, 2006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data1'].groupby([states, years]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently the grouping information to be found in the same DataFrame as the data we want to work on. In that case, we can pass column names as the group keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('key1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['key1', 'key2']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may have noticed in the first case **df.groupby('key1').mean()** that there is no **key2** column in the result. Because **df['key2']** is not numeric data, it is said to be a *nuisance column*, which is therefore excluded from the result. By default, all of the numeric columns are aggregated.\n",
    "\n",
    "Regardless of the objective in using **groupby**, a generally useful GroupBy method is **size** which return a Series containing group sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.groupby(['key1', 'key2']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GroupBy Mechanics* >\n",
    "### Iterating Over Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GroupBy object supports iteration, generating a sequence of 2-tuples containing the group name along with the chunk of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df.groupby('key1'):\n",
    "    print(name)\n",
    "    print(type(group))\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of multiple keys, the first element in the tuple will be a tuple of key values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k1, k2), group in df.groupby(['key1', 'key2']):\n",
    "    print(k1, k2)\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose to do whatever we want with the pieces of data. A recipe we may find useful is computing a dict of the data pieces as a one-liner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = dict(list(df.groupby('key1')))\n",
    "pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default **groupby** groups on axis=0, but you can group on any of the other axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(df.dtypes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(list(grouped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GroupBy Mechanics* >\n",
    "### Selecting a Column or Subset of Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing a GroupBy object created from a DataFrame with a column name or array of column names has the effect of *selecting those columns* for aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('key1')['data1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data1'].groupby(df['key1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially for large data sets, it may be desirable to aggregate only a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['key1', 'key2'])[['data2']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object returned by this indexing operation is a grouped DataFrame if a list or array is passed and a grouped Series is just a single column name that is passed as a scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_grouped = df.groupby(['key1', 'key2'])['data2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(list(s_grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_grouped.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GroupBy Mechanics* >\n",
    "### Grouping with Dicts and Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping information may exist in a form other than an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = DataFrame(np.random.randn(5, 5), \n",
    "                   columns=['a', 'b', 'c', 'd', 'e'], \n",
    "                   index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])\n",
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.loc[2:3, ['b', 'c']] = np.nan # Add a few NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a group correspondence for the columns and want to sum together the columns by group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'a': 'red', 'b': 'red', 'c': 'blue', \n",
    "           'd': 'blue', 'e': 'red', 'f' : 'orange'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could easily construct an array from this dict to pass to **groupby**, but instead we can just pass the dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_column = people.groupby(mapping, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(list(by_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_column.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same functionality holds for Series, which can be viewed as a fixed size mapping. When we used Series as group keys in the above examples, pandas does, in fact, inspect each Series to ensure that its index is aligned with the axis it’s grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_series = Series(mapping)\n",
    "map_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.groupby(map_series, axis=1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GroupBy Mechanics* >\n",
    "### Grouping with Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python functions in what can be fairly creative ways is a more abstract way of defining a group mapping compared with a dict or Series. Any function passed as a group key will be called once per index value, with the return values being used as the group names.\n",
    "\n",
    "Suppose we wanted to group by the length of the names; we could compute an array of string lengths, but instead we can just pass the **len** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.groupby(len).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixing functions with arrays, dicts, or Series is not a problem as everything gets converted to arrays internally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = ['one', 'one', 'one', 'two', 'two']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.groupby([len, key_list]).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GroupBy Mechanics* >\n",
    "### Grouping by Index Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final convenience for hierarchically-indexed data sets is the ability to aggregate using one of the levels of an axis index. To do this, pass the level number or name using the level keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],\n",
    "                                     [1, 3, 5, 1, 3]], names=['cty', 'tenor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_df = DataFrame(np.random.randn(4, 5), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_df.groupby(level='tenor', axis=1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By aggregation, It’s generally referred to any data transformation that produces scalar values from arrays. In the examples above we have used several of them, such as **mean**, **count**, **min** and **sum**.\n",
    "\n",
    "Many common aggregations have optimized implementations that compute the statistics on the dataset in place. However, we are not limited to only this set of methods.\n",
    "\n",
    "We can use aggregations of our own devising and additionally call any method that is also defined on the grouped object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('key1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped['data1'].quantile(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While **quantile** is not explicitly implemented for GroupBy, it is a Series method and thus available for use. Internally, GroupBy efficiently slices up the Series, calls **piece.quantile(0.9)** for each piece, then assembles those results together into the result object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use our own aggregation functions, pass any function that aggregates an array to the aggregate or agg method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_to_peak(arr):\n",
    "    return arr.max() - arr.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.agg(peak_to_peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll notice that some methods like describe also work, even though they are not aggregations, strictly speaking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Pics/picture_5_2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data Aggregation* >\n",
    "### Column-wise and Multiple Function Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to aggregate using a different function depending on the column or multiple functions at once. Fortunately, this is straightforward to do, which is illustrated through a number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = pd.read_csv('./Data/tips.csv')\n",
    "tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tip percentage of total bill\n",
    "tips['tip_pct'] = tips['tip'] / tips['total_bill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass the name of the function as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = tips.groupby(['sex', 'smoker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_pct = grouped['tip_pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_pct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_pct.agg('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass a list of functions or function names instead, we get back a DataFrame with column names taken from the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_pct.agg(['mean', 'std', peak_to_peak])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don’t need to accept the names that GroupBy gives to the columns; notably lambda functions have the name '<**lambda**\\>' which make them hard to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_pct.agg([('foo', 'mean'), ('bar', np.std)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a DataFrame, you have more options as you can specify a list of functions to apply to all of the columns or different functions per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = ['count', 'mean', 'max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = grouped['tip_pct', 'total_bill'].agg(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting DataFrame has hierarchical columns, the same as you\n",
    "would get aggregating each column separately and using concat to glue the results together using the column names as the keys argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['tip_pct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of tuples with custom names can be passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftuples = [('Durchschnitt', 'mean'), ('Abweichung', np.var)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped['tip_pct', 'total_bill'].agg(ftuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to apply potentially different functions to one or more of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.agg({'tip' : np.max, 'size' : 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.agg({'tip_pct' : ['min', 'max', 'mean', 'std'], \n",
    "             'size' : 'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data Aggregation* >\n",
    "### Returning Aggregated Data in “unindexed” Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregated data comes back with an index, potentially hierarchical, composed from the unique group key combinations observed.\n",
    "\n",
    "We can disable this behavior in most cases by passing **as_index=False** to **groupby**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.groupby(['sex', 'smoker'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group-wise Operations and Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation is only one kind of group operation. It is a special case in the more general class of data transformations; that is, it accepts functions that reduce a one-dimensional array to a scalar value. \n",
    "Introduce to the **transform** and **apply** methods, which will enable us to do many other kinds of group operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1_means = df.groupby('key1').mean().add_prefix('mean_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df, k1_means, left_on='key1', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but is somewhat inflexible. We can think of the operation as transforming the two data columns using the **np.mean** function. Let’s look back at the **people** Data-\n",
    "Frame from earlier in the chapter and use the **transform** method on GroupBy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = ['one', 'two', 'one', 'two', 'one']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.groupby(key).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.groupby(key).transform(np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform** applies a function to each group, then places the results in the appropriate locations. If each group produces a scalar value, it will be propagated. \n",
    "\n",
    "Suppose instead we wanted to subtract the mean value from each group. To do this, create a demeaning function and pass it to transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demean(arr):\n",
    "    return arr - arr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demeaned = people.groupby(key).transform(demean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demeaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that demeaned now has zero group means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demeaned.groupby(key).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Group-wise Operations and Transformations* >\n",
    "### Apply: General split-apply-combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform** is a more specialized function having rigid requirements: the passed function must either produce a scalar value to be broadcasted (like **np.mean**) or a transformed array of the same size.\n",
    "\n",
    "The most general purpose GroupBy method is **apply**. **apply** splits the object being manipulated into pieces, invokes the passed function on each piece, then attempts to concatenate the pieces together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to select the top five **tip_pct** values by group. First, it’s straightforward to write a function that selects the rows with the largest values in a particular column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top(df, n=5, column='tip_pct') :\n",
    "    return df.sort_values(by=column)[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top(tips, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we group by smoker, say, and call apply with this function, we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.groupby('smoker').apply(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **top** function is called on each piece of the DataFrame,\n",
    "then the results are glued together using **pandas.concat**, labeling the pieces with the group names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass a function to apply that takes other arguments or keywords, we can pass these after the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may recall above **describe** on a GroupBy object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tips.groupby('smoker')['tip_pct'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.unstack('smoker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Group-wise Operations and Transformations* > *Apply: General split-apply-combine* >\n",
    "#### Suppressing the group keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the resulting object has a hierarchical index formed\n",
    "from the group keys along with the indexes of each piece of the original object. This can be disabled by passing **group_keys=False** to **groupby**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.groupby('smoker', group_keys=False).apply(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Group-wise Operations and Transformations* >\n",
    "### Quantile and Bucket Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas has some tools, in particular **cut** and **qcut**, for slicing data up into buckets with bins of your choosing or by sample quantiles. Combining these functions with **groupby**, it becomes very simple to perform bucket or quantile analysis on a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = DataFrame({'data1': np.random.randn(1000), \n",
    "                   'data2': np.random.randn(1000)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = pd.cut(frame.data1, 4)\n",
    "factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Factor** object returned by **cut** can be passed directly to **groupby**. So we could compute a set of statistics for the **data2** column like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(group) :\n",
    "    return {'min': group.min(), 'max': group.max(),\n",
    "           'count': group.count(), 'mean': group.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = frame.data2.groupby(factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.apply(get_stats).unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were equal-length buckets; to compute equal-size buckets based on sample quantiles, use **qcut** and pass **labels=False** to just get quantile numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping = pd.qcut(frame.data1, 10, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = frame.data2.groupby(grouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.apply(get_stats).unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Group-wise Operations and Transformations* >\n",
    "### Example: Filling Missing Values with Group-specific Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When cleaning up missing data, in some cases you will filter out data observations using **dropna**, but in others you may want to impute (fill in) the NA values using a fixed value or some value derived from the data. **fillna** is the right tool to use;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Series(np.random.randn(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[::2] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.fillna(s.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we need the fill value to vary by group. As we may guess, we need only group the data and use apply with a function that calls **fillna** on each data chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['Ohio', 'New York', 'Vermont', 'Florida', \n",
    "          'Oregon', 'Nevada', 'California', 'Idaho']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_key = ['East']*4 + ['West']*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Series(np.random.randn(8), index=states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Vermont', 'Nevada', 'Idaho']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(group_key).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill the NA values using the group means like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mean = lambda g : g.fillna(g.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(group_key).apply(fill_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might have pre-defined fill values in our code that vary by group. Since the groups have a name attribute set internally, we can use that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_values = {'East': 0.5, 'West': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_func = lambda g: g.fillna(fill_values[g.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(group_key).apply(fill_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Group-wise Operations and Transformations* >\n",
    "### Example: Random Sampling and Permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to draw a random sample (with or without replacement) from a large dataset for Monte Carlo simulation purposes or some other application. There are a number of ways to perform the “draws”;\n",
    "\n",
    "One way is to select the first **K** elements of  **np.random.permutation(N)**, where **N** is the size of your complete dataset and **K** the desired sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suits = ['H', 'S', 'C', 'D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_val = (list(range(1, 11)) + [10]*3)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_names = ['A'] + list(range(2, 11)) + ['J', 'K', 'Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suit in ['H', 'S', 'C', 'D']: \n",
    "    cards.extend(str(num) + suit for num in base_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck = Series(card_val, index=cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a Series of length 52 whose index contains card names and values are the ones used in blackjack and other games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck[:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what described above, draw a hand of 5 cards from the desk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(deck, n=5) :\n",
    "    return deck.take(np.random.permutation(len(deck))[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(deck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted two random cards from each suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_suit = lambda card: card[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck.groupby(get_suit).apply(draw, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck.groupby(get_suit, group_keys=False).apply(draw, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Group-wise Operations and Transformations* >\n",
    "### Example: Group Weighted Average and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the split-apply-combine paradigm of **groupby**, operations between columns in a DataFrame or two Series, such a group weighted average, become a routine affair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame({'category': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'], \n",
    "                'data': np.random.randn(8),\n",
    "                'weights': np.random.rand(8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The group weighted average by category would then be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_wavg = lambda g: np.average(g['data'], weights=g['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.apply(get_wavg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a data set from Yahoo! Finance containing end of day prices for a few stocks and the S&P 500 index (the SPX ticker):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_px = pd.read_csv('./Data/stock_px.csv', parse_dates=True, \n",
    "                       index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_px.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_px[-4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One task of interest might be to compute a DataFrame consisting of the yearly correlations of daily returns (computed from percent changes) with SPX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets = close_px.pct_change().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_corr = lambda x: x.corrwith(x['SPX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_year = rets.groupby(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_year.apply(spx_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_year.apply(lambda g: g['AAPL'].corr(g['MSFT']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Group-wise Operations and Transformations* >\n",
    "### Example: Group-wise Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use groupby to perform more complex group-wise statistical analysis, as long as the function returns a pandas object or scalar value. For example, we can define the following **regress** function (using the **statsmodels** econometrics library) which executes an ordinary least squares (OLS) regression on each chunk of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress(data, yvar, xvars):\n",
    "    Y = data[yvar]\n",
    "    X = data[xvars] \n",
    "    X['intercept'] = 1.\n",
    "    result = sm.OLS(Y, X).fit()\n",
    "    return result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_year.apply(regress, 'AAPL', ['SPX'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Tables and Cross-Tabulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *pivot* table is a data summarization tool frequently found in spreadsheet programs and other data analysis software. It aggregates a table of data by one or more keys, arranging the data in a rectangle with some of the group keys along the rows and some along the columns. \n",
    "\n",
    "Pivot tables in Python with pandas are made possible using the **groupby** facility combined with reshape operations utilizing hierarchical indexing.\n",
    "\n",
    "DataFrame has a **pivot_table** method, and additionally there is a top-level **pandas.pivot_table** function. \n",
    "\n",
    "In addition to providing a convenience interface to **groupby**, **pivot_table** also can add partial totals, also known as *margins*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to compute a table of group means (the default **pivot_table** aggregation type) arranged by **sex** and **smoker** on the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table(index = ['sex', 'smoker'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could have been easily produced using **groupby**. Now, suppose we want to aggregate only **tip_pct** and **size**, and additionally group by **day**. We’ll put **smoker** in the table columns and **day** in the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table(['tip_pct', 'size'],\n",
    "                index = ['sex', 'day'],\n",
    "                columns = 'smoker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table could be augmented to include partial totals by passing **margins=True**. This has the effect of adding **All** row and column labels, with corresponding values being the group statistics for all the data within a single tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table(['tip_pct', 'size'],\n",
    "                index = ['sex', 'day'],\n",
    "                columns = 'smoker', margins = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a different aggregation function, pass it to **aggfunc**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table('tip_pct', index = ['sex', 'smoker'], columns = 'day', \n",
    "                aggfunc = len, margins = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some combinations are empty (or otherwise NA), we may wish to pass a **fill_value**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table('size', index = ['time', 'sex', 'smoker'], columns = 'day', \n",
    "                aggfunc = 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table('size', index = ['time', 'sex', 'smoker'], columns = 'day', \n",
    "                aggfunc = 'sum', fill_value = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Pics/picture_5_3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pivot Tables and Cross-Tabulation* >\n",
    "### Cross-Tabulations: Crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cross-tabulation (or *crosstab* for short) is a special case of a pivot table that computes group frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_data = {'Sample': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "              'Gender': ['Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Female'], \n",
    "              'Handedness': ['Right-handed', 'Left-handed', 'Right-handed', 'Right-handed', 'Left-handed', \n",
    "                             'Right-handed', 'Right-handed', 'Left-handed', 'Right-handed', 'Right-handed']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame(cross_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.Gender, data.Handedness, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two arguments to crosstab can each either be an array or Series or a list of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab([tips.time, tips.day], tips.smoker, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: 2012 Federal Election Commission Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The US Federal Election Commission publishes data on contributions to political campaigns. This includes contributor names, occupation and employer, address, and contribution amount. An interesting dataset is from the 2012 US presidential election (http://www.fec.gov/disclosurep/PDownload.do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fec = pd.read_csv('./Data/P00000001-ALL.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fec.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fec.loc[123456]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no political party affiliations in the data, so this would be useful to add. We can get a list of all the unique political candidates using **unique**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cands = fec.cand_nm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cands[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties = {'Bachmann, Michelle': 'Republican', \n",
    "           'Cain, Herman': 'Republican',\n",
    "           'Gingrich, Newt': 'Republican', \n",
    "           'Huntsman, Jon': 'Republican', \n",
    "           'Johnson, Gary Earl': 'Republican', \n",
    "           'McCotter, Thaddeus G': 'Republican', \n",
    "           'Obama, Barack': 'Democrat',\n",
    "           'Paul, Ron': 'Republican',\n",
    "           'Pawlenty, Timothy': 'Republican',\n",
    "           'Perry, Rick': 'Republican',\n",
    "           \"Roemer, Charles E. 'Buddy' III\": 'Republican', \n",
    "           'Romney, Mitt': 'Republican', \n",
    "           'Santorum, Rick': 'Republican'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this mapping and the map method on Series objects, you can compute an array of political parties from the candidate names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fec.cand_nm[123456:123461]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fec.cand_nm[123456:123461].map(parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fec['party'] = fec.cand_nm.map(parties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fec['party'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of data preparation points. First, this data includes both contributions and refunds (negative contribution amount):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fec.contb_receipt_amt > 0).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of data preparation points. First, this data includes both contributions and refunds (negative contribution amount):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fec = fec[fec.contb_receipt_amt > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Barack Obama and Mitt Romney are the main two candidates, we’ll also prepare a subset that just has contributions to their campaigns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fec_mrbo = fec[fec.cand_nm.isin(['Obama, Barack', 'Romney, Mitt'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example: 2012* >\n",
    "### Donation Statistics by Occupation and Employer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donations by occupation is another oft-studied statistic. First, the total number of donations by occupation is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fec.contbr_occupation.value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will notice by looking at the occupations that many refer to the same basic job type, or there are several variants of the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_mapping = {'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED', \n",
    "               'INFORMATION REQUESTED' : 'NOT PROVIDED', \n",
    "               'INFORMATION REQUESTED (BEST EFFORTS)' : 'NOT PROVIDED', \n",
    "               'C.E.O.': 'CEO'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no mapping provided, return x\n",
    "f = lambda x: occ_mapping.get(x, x) \n",
    "fec.contbr_occupation = fec.contbr_occupation.map(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll also do the same thing for employers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_mapping = {'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED', \n",
    "               'INFORMATION REQUESTED' : 'NOT PROVIDED', \n",
    "               'SELF' : 'SELF-EMPLOYED', \n",
    "               'SELF EMPLOYED' : 'SELF-EMPLOYED'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no mapping provided, return x\n",
    "f = lambda x: emp_mapping.get(x, x) \n",
    "fec.contbr_employer = fec.contbr_employer.map(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **pivot_table** to aggregate the data by party and occupation, then filter down to the subset that donated at least $2 million overall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_occupation = fec.pivot_table('contb_receipt_amt', \n",
    "                                index='contbr_occupation',\n",
    "                                columns='party', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_2mm = by_occupation[by_occupation.sum(1) > 2000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_2mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be easier to look at this data graphically as a bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_2mm.plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be interested in the top donor occupations or top companies donating to Obama and Romney."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_amounts(group, key, n=5):\n",
    "    totals = group.groupby(key)['contb_receipt_amt'].sum()\n",
    "    \n",
    "    # Order totals by key in descending order \n",
    "    return totals.sort_values(ascending=False)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = fec_mrbo.groupby('cand_nm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.apply(get_top_amounts, 'contbr_employer', n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example: 2012* >\n",
    "### Bucketing Donation Amounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful way to analyze this data is to use the cut function to discretize the contributor amounts into buckets by contribution size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.array([0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.cut(fec_mrbo.contb_receipt_amt, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = fec_mrbo.groupby(['cand_nm', labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.size().unstack()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This data shows that Obama has received a significantly larger number of small donations than Romney. We can also sum the contribution amounts and normalize within buckets to visualize percentage of total donations of each size by candidate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_sums = grouped.contb_receipt_amt.sum().unstack(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_sums = bucket_sums.div(bucket_sums.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_sums[:-2].plot(kind='barh', stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise\n",
    "\n",
    "You could aggregate donations by donor name and zip code to adjust for donors who gave many small amounts versus one or more large donations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example: 2012* >\n",
    "### Donation Statistics by State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating the data by candidate and state is a routine affair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = fec_mrbo.groupby(['cand_nm', 'contbr_st'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = grouped.contb_receipt_amt.sum().unstack(0).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = totals[totals.sum(1) > 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we divide each row by the total contribution amount, we get the relative percentage of total donations by state for each candidate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = totals.div(totals.sum(1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise: Plot this data on a map.\n",
    "\n",
    "After locating a shape file for the state boundaries (http://nationalmap.gov/small_scale/atlasftp.html?openChapters=chpbound#chpbound) and learning a bit more about matplotlib and its basemap toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Pics/picture_5_4.png'>\n",
    "<img src='./Pics/picture_5_5.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyshp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install GEOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!conda install basemap  # in shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpl_toolkits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.collections import LineCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama = percent['Obama, Barack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = fig.add_axes([0.1,0.1,0.8,0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lllat = 21; urlat = 53; lllon = -118; urlon = -62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Basemap(ax=ax, projection='stere',\n",
    "            lon_0=(urlon + lllon) / 2, lat_0=(urlat + lllat) / 2, \n",
    "            llcrnrlat=lllat, urcrnrlat=urlat, llcrnrlon=lllon, urcrnrlon=urlon, resolution='l')\n",
    "m.drawcoastlines() \n",
    "m.drawcountries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = shapefile.Reader('../Data/statesp020_nt00032/statesp020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = shp.shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = shp.records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_code = {\n",
    "        'Alaska': 'AK',\n",
    "        'Alabama': 'AL',\n",
    "        'Arkansas': 'AR',\n",
    "        'American Samoa': 'AS',\n",
    "        'Arizona': 'AZ',\n",
    "        'California': 'CA',\n",
    "        'Colorado': 'CO',\n",
    "        'Connecticut': 'CT',\n",
    "        'District of Columbia': 'DC',\n",
    "        'Delaware': 'DE',\n",
    "        'Florida': 'FL',\n",
    "        'Georgia': 'GA',\n",
    "        'Guam': 'GU',\n",
    "        'Hawaii': 'HI',\n",
    "        'Iowa': 'IA',\n",
    "        'Idaho': 'ID',\n",
    "        'Illinois': 'IL',\n",
    "        'Indiana': 'IN',\n",
    "        'Kansas': 'KS',\n",
    "        'Kentucky': 'KY',\n",
    "        'Louisiana': 'LA',\n",
    "        'Massachusetts': 'MA',\n",
    "        'Maryland': 'MD',\n",
    "        'Maine': 'ME',\n",
    "        'Michigan': 'MI',\n",
    "        'Minnesota': 'MN',\n",
    "        'Missouri': 'MO',\n",
    "        'Northern Mariana Islands': 'MP',\n",
    "        'Mississippi': 'MS',\n",
    "        'Montana': 'MT',\n",
    "        'National': 'NA',\n",
    "        'North Carolina': 'NC',\n",
    "        'North Dakota': 'ND',\n",
    "        'Nebraska': 'NE',\n",
    "        'New Hampshire': 'NH',\n",
    "        'New Jersey': 'NJ',\n",
    "        'New Mexico': 'NM',\n",
    "        'Nevada': 'NV',\n",
    "        'New York': 'NY',\n",
    "        'Ohio': 'OH',\n",
    "        'Oklahoma': 'OK',\n",
    "        'Oregon': 'OR',\n",
    "        'Pennsylvania': 'PA',\n",
    "        'Puerto Rico': 'PR',\n",
    "        'Rhode Island': 'RI',\n",
    "        'South Carolina': 'SC',\n",
    "        'South Dakota': 'SD',\n",
    "        'Tennessee': 'TN',\n",
    "        'Texas': 'TX',\n",
    "        'Utah': 'UT',\n",
    "        'Virginia': 'VA',\n",
    "        'Virgin Islands': 'VI',\n",
    "        'Vermont': 'VT',\n",
    "        'Washington': 'WA',\n",
    "        'Wisconsin': 'WI',\n",
    "        'West Virginia': 'WV',\n",
    "        'Wyoming': 'WY'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record, shape in zip(records, shapes):\n",
    "    lons,lats = zip(*shape.points)\n",
    "    data = np.array(m(lons, lats)).T\n",
    "    \n",
    "    try:\n",
    "        per = obama[state_to_code[record[3]]]\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "    lines = LineCollection([data,], antialiaseds=(1,))\n",
    "    lines.set_facecolors('k')\n",
    "    lines.set_alpha(0.75 * per)\n",
    "    lines.set_edgecolors('k')\n",
    "    lines.set_linewidth(0.3)\n",
    "    ax.add_collection(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
